{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## several scripts to verify and manipulate file names in the S3 bepress dump  \n",
    "\n",
    "NOTE: must have boto3 module installed in python environment. You must \n",
    "\n",
    "- modify the third cell to include your values for the S3 bucket\n",
    "- run the first three cells for all other scripts to work\n",
    "\n",
    "### The main scripts in the notebook\n",
    "\n",
    "- [Recursive function to explore folders in the S3 bucket](#explore)\n",
    "- [Get OAI and add S3 filenames as &lt;fulltext-s3-path&gt; in xml](#rename) \n",
    "- [Restructure OAI-PMH xml as json](#xmltojson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ##\n",
    " # Get credentials for the S3 bucket and login. Credentials are stored in a plain text file as \n",
    " # access key and secret key both on one line separated by the '|' character. The file must\n",
    " # have a second blank line (two lines total)\n",
    "##\n",
    "authfile = open(\"credentials.txt\", \"r\")\n",
    "[ACCESS_KEY,SECRET_KEY] = authfile.readline().strip().split('|')\n",
    "authfile.close()\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=ACCESS_KEY,\n",
    "    aws_secret_access_key=SECRET_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ##\n",
    " # Set some variables that get reused a lot (parameters)\n",
    "##\n",
    "\n",
    "# folders to ignore\n",
    "to_ignore = [\n",
    "    'archive/scholarlyrepository.miami.edu/anthurium/',\n",
    "    'archive/scholarlyrepository.miami.edu/collaborations/',\n",
    "    'archive/scholarlyrepository.miami.edu/collaborations_slideshow/'\n",
    "]\n",
    "\n",
    "# your bucket name here\n",
    "bucket = 'bepressarchivemiami'\n",
    "\n",
    "# your prefix here\n",
    "prefix = 'archive/scholarlyrepository.miami.edu/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ##\n",
    " # Read through the top level of the bucket\n",
    "##\n",
    "top_level_folders = s3.list_objects(\n",
    "    Bucket=bucket,\n",
    "    Prefix=prefix,\n",
    "    Delimiter='/'\n",
    ")\n",
    "top_level_folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"explore\"></a>\n",
    "\n",
    "## Function to explore folders on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    " # Function to recurse over folders and list content as per rules we define\n",
    " ##\n",
    "\n",
    "def explore_folder(bucket, folder_prefix, to_ignore):\n",
    "    \n",
    "    if folder_prefix in to_ignore:\n",
    "        return\n",
    "    \n",
    "    print (folder_prefix)\n",
    "    objects = s3.list_objects(\n",
    "        Bucket=bucket,\n",
    "        Prefix=folder_prefix,\n",
    "        Delimiter='/'\n",
    "    )\n",
    "    \n",
    "    if 'CommonPrefixes' in objects: # there are more folders\n",
    "        \n",
    "        folders = objects['CommonPrefixes']\n",
    "        if objects['IsTruncated']: # make sure to get all pages (max 1000 per page)\n",
    "            kwargs = {\n",
    "                'Bucket': bucket,\n",
    "                'Prefix': folder_prefix,\n",
    "                'Delimiter': '/',\n",
    "                'Marker': objects['NextMarker']\n",
    "            }\n",
    "            while True:\n",
    "                objects = s3.list_objects(**kwargs)\n",
    "                for folder in objects['CommonPrefixes']:\n",
    "                    folders.append(folder)\n",
    "                    \n",
    "                try:\n",
    "                    kwargs['Marker'] = objects['NextMarker']\n",
    "                except KeyError:\n",
    "                    break\n",
    "                    \n",
    "        for folder in folders:\n",
    "            explore_folder(bucket,folder['Prefix'],to_ignore) # recursive call to step throught folder structure\n",
    "            \n",
    "    else: # we are at the content level, print the list of files\n",
    "        internal_files = s3.list_objects(Bucket=bucket,Prefix=folder_prefix)\n",
    "        for file in internal_files['Contents']:\n",
    "            # just some extra parsing\n",
    "            path = file['Key'].split('/')\n",
    "            filename = path[len(path)-1]\n",
    "            extension = filename.split('.')[-1]\n",
    "            foldernumber = path[len(path)-2]\n",
    "            path = \"/\".join(path[:-2]) + \"/\"\n",
    "            star = ''\n",
    "            # this next line os for ETDs and Theses, it will be different for other folders\n",
    "            # it simply highlights the main download\n",
    "            if not ('stamped' in file['Key'] or 'metadata' in file['Key']):\n",
    "                star = ' *'\n",
    "\n",
    "            print (path+filename+star)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the function on some folder. Note: this could be run at the top level, but it will take a while ...\n",
    "# for example:\n",
    "explore_folder(\n",
    "    bucket, \n",
    "    'archive/scholarlyrepository.miami.edu/socomm_cinema_scripts/211',\n",
    "    to_ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if we can download a file\n",
    "with open('vpk6wuij76lfpov4cdt3c7k3ur9skvxe.pdf', 'wb') as f:\n",
    "    s3.download_fileobj('bepressarchivemiami', 'archive/scholarlyrepository.miami.edu/socomm_cinema_scripts/199/wcehn5pxu7a9l6a1y81e49wynl5tn3mn.pdf', f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"rename\"></a>\n",
    "\n",
    "## Get OAI and add S3 filenames as &lt;fulltext-s3-path&gt; in xml\n",
    "\n",
    "This is the main tool in this notebook. It steps through the OAI-PMH feed from bepress and then finds the corresponding filenames on S3 for all restricted materials and adds the element &lt;fulltext-s3-path&gt; to the xml.\n",
    "\n",
    "The output is a set of xml files as pages in the 'pages' folder\n",
    "    \n",
    "Other output notes:\n",
    "\n",
    "- this script prints the current page number for each step\n",
    "- the script will print an error message if no files were found in the S3 bucket\n",
    "- the script creates an 'errors' array that holds a list of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the bepress oai endpoint with any qualifiers for the request\n",
    "url = \"https://scholarlyrepository.miami.edu/do/oai/?verb=ListRecords\"\n",
    "qualifier = \"&metadataPrefix=document-export&set=publication:um_research_publications\"\n",
    "\n",
    "# an array with a list of known restricted series in bepress\n",
    "known_restricted = [\n",
    "    \"Master of Fine Arts Creative Writing Theses\",\n",
    "    \"Internship Reports (Restricted)\",\n",
    "    \"Scriptwriting Senior Projects (Restricted)\",\n",
    "    \"Archived Data Sets (Restricted)\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import datetime\n",
    "\n",
    "# write all xml pages to this directory\n",
    "output_directory = \"pages/\"\n",
    "errors = []\n",
    "\n",
    "###\n",
    " # step through xml page and create <fulltext-s3-path> elements with a filename\n",
    " # for any restricted documents on bepress\n",
    " ##\n",
    "def updateXML(doclist):\n",
    "    for record in doclist.findall('{http://www.openarchives.org/OAI/2.0/}record'):\n",
    "        \n",
    "        # get document and some of the meta\n",
    "        document = record.find(\n",
    "            '{http://www.openarchives.org/OAI/2.0/}metadata/document-export/documents/document'\n",
    "        )\n",
    "        path = prefix + document.find('submission-path').text + '/'\n",
    "        internal_files = s3.list_objects(Bucket=bucket,Prefix=path)\n",
    "        restricted = False\n",
    "        doc_type = document.find('document-type').text\n",
    "        pub_title = document.find('publication-title').text\n",
    "        \n",
    "        # check to see if document is restricted\n",
    "        if doc_type == 'restricted': \n",
    "            restricted = True; \n",
    "        elif doc_type == 'withheld':\n",
    "            got_date = False\n",
    "            date = \"\"\n",
    "            for field in document.findall('fields/field'):\n",
    "                field_type = field.get('name')\n",
    "                if field_type == \"embargo_date\":\n",
    "                    date = field.find('value').text\n",
    "                    if date > datetime.datetime.today().strftime('%Y-%m-%d'):\n",
    "                        restricted = True\n",
    "                        got_date = True\n",
    "        elif pub_title in known_restricted:\n",
    "            restricted = True;\n",
    "            \n",
    "        # if needed add the <fulltext-s3-path> element\n",
    "        if 'Contents' in internal_files:\n",
    "            for file in internal_files['Contents']:\n",
    "                # this next line os for ETDs and Theses, it may be different for other folders\n",
    "                if not ('stamped' in file['Key'] or 'metadata' in file['Key']):\n",
    "                    filename = file['Key'].split('/')[-1]\n",
    "                    if filename == '':\n",
    "                        print ('ERROR: no download file for ' + path)\n",
    "                        errors.append('ERROR: no download file for: ' + path)\n",
    "                    # as requested, only provide new element for restricted items\n",
    "                    if restricted:\n",
    "                        s3_path = ET.SubElement(document,'fulltext-s3-path')\n",
    "                        s3_path.text = path + filename\n",
    "                        \n",
    "        # print error if no file was found\n",
    "        else:\n",
    "            print ('ERROR: no files at all for: ' + path)\n",
    "            errors.append('ERROR: no files at all for: ' + path)\n",
    "\n",
    "            \n",
    "###\n",
    " # Start of main script\n",
    " ##\n",
    "\n",
    "ET.register_namespace('oai',\"http://www.openarchives.org/OAI/2.0/\")\n",
    "page = 1\n",
    "print ('page:',page)\n",
    "\n",
    "# make the first request\n",
    "response = requests.get(url+qualifier)\n",
    "allXML = ET.fromstring(response.content.decode('utf-8'))\n",
    "\n",
    "# get the first page and the resumption token for the next page\n",
    "documentlist = allXML.find('{http://www.openarchives.org/OAI/2.0/}ListRecords')\n",
    "resume = documentlist.find('{http://www.openarchives.org/OAI/2.0/}resumptionToken')\n",
    "documentlist.remove(resume)\n",
    "resume = resume.text\n",
    "\n",
    "# add <fulltext-s3-path> elements as needed\n",
    "updateXML(documentlist)\n",
    "\n",
    "# write the first page to a file\n",
    "root = ET.ElementTree(allXML)\n",
    "today = datetime.datetime.now().isoformat()[0:10].replace('-', '')\n",
    "root.write(output_directory+today+'_OAI_page'+str(page)+'.xml',encoding=\"utf-8\",xml_declaration=True)\n",
    "\n",
    "# loop through the rest of the pages\n",
    "while not resume == None:\n",
    "    page += 1\n",
    "    print ('page:',page)\n",
    "    \n",
    "    # make the page request\n",
    "    response = requests.get(url+'&resumptionToken='+resume)\n",
    "    pageXML = ET.fromstring(response.content.decode('utf-8'))\n",
    "    \n",
    "    # get the page and the resumption token\n",
    "    pagelist = pageXML.find('{http://www.openarchives.org/OAI/2.0/}ListRecords')\n",
    "    resume = pagelist.find('{http://www.openarchives.org/OAI/2.0/}resumptionToken')\n",
    "    pagelist.remove(resume)\n",
    "    resume = resume.text\n",
    "    \n",
    "    # add <fulltext-s3-path> elements as needed\n",
    "    updateXML(pagelist)\n",
    "    \n",
    "    # write page to file\n",
    "    root = ET.ElementTree(pageXML)\n",
    "    root.write(output_directory+today+'_OAI_page'+str(page)+'.xml',encoding=\"utf-8\",xml_declaration=True)\n",
    "    \n",
    "    # add page to entire xml document (no pages)\n",
    "    documentlist.extend(pagelist)\n",
    "    \n",
    "# write entire xml document to file (no pages)\n",
    "root = ET.ElementTree(allXML)\n",
    "root.write(output_directory+today+'_OAI_bepress.xml',encoding=\"utf-8\",xml_declaration=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"xmltojson\"></a>\n",
    "\n",
    "## step through OAI data from bepress and restructure\n",
    "\n",
    "Just for exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "# paste your xml file here\n",
    "with open('pages/20190925_OAI_bepress.xml', 'r') as xml_file:\n",
    "    rootDocs = ET.parse(xml_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = {}\n",
    "documents = rootDocs.find('{http://www.openarchives.org/OAI/2.0/}ListRecords').findall('{http://www.openarchives.org/OAI/2.0/}record')\n",
    "for document in documents:\n",
    "    container = document.find('{http://www.openarchives.org/OAI/2.0/}metadata').find('document-export').find('documents').find('document')\n",
    "    path = container.find('submission-path').text\n",
    "    folders = path.split('/')\n",
    "    if folders[0] in directories:\n",
    "        directories[folders[0]]['count'] += 1\n",
    "    else:\n",
    "        if not folders[0] in to_ignore:\n",
    "            directories[folders[0]] = {}\n",
    "            directories[folders[0]]['count'] = 1\n",
    "            directories[folders[0]]['items'] = []\n",
    "    \n",
    "    if not len(folders) == 2: # simple numeric file structure\n",
    "        print (folders[0])\n",
    "    else:\n",
    "        item = {\n",
    "            'title': ''.join(e for e in container.find('title').text.split() if e.isalnum()),\n",
    "            'year': container.find('publication-date').text[0:4],\n",
    "            'author': ''\n",
    "        }\n",
    "        try:\n",
    "            item['author'] = container.find('authors').find('author').find('lname').text\n",
    "        except:\n",
    "            try:\n",
    "                item['author'] = container.find('authors').find('author').find('organization').text\n",
    "            except:     \n",
    "                try:\n",
    "                    item['author'] = container.find('authors').find('author').find('institution').text\n",
    "                except:\n",
    "                    print ('woof')\n",
    "            \n",
    "        directories[folders[0]]['items'].append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
